{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EjcpcKwppPJC"
   },
   "source": [
    "\n",
    "# Advanced Certification in AIML\n",
    "## A Program by IIIT-H and TalentSprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ksg-aNjbn2h0"
   },
   "source": [
    "## Learning Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dW2P6nr1n6fv"
   },
   "source": [
    "At the end of the experiment, you will be able to:\n",
    "\n",
    "\n",
    "*  Apply K-Fold cross-validation method\n",
    "*  Tune the hyperparameters of MLP Classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ovdZ_4tln7Oz"
   },
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZbclXs06n8-b"
   },
   "source": [
    "### Description:\n",
    "\n",
    "The MNIST dataset contains: \n",
    "\n",
    "1. 60,000 Handwritten digits as training samples and 10,000 Test samples, \n",
    "which means each digit occurs 6000 times in the training set and 1000 times in the testing set. (approximately). \n",
    "2. Each image is Size Normalized and Centered \n",
    "3. Each image is 28 X 28 Pixel with 0-255 Gray Scale Value. \n",
    "4. That means each image is represented as 784 (28 X28) dimension vector where each value is in the range 0- 255.\n",
    "\n",
    "\n",
    "\n",
    "### History\n",
    "\n",
    "Yann LeCun (Director of AI Research, Facebook, Courant Institute, NYU) was given the task of identifying the cheque numbers (in the ’90s) and the amount associated with that cheque without manual intervention. That is when this dataset was created which raised the bars and became a benchmark.\n",
    "\n",
    "Yann LeCun and Corinna Cortes (Google Labs, New York) hold the copyright of the MNIST dataset, which is a subset of the original NIST datasets. This dataset is made available under the terms of the Creative Commons Attribution-Share Alike 3.0 license.\n",
    "\n",
    "It is the handwritten digits dataset in which half of them are written by the Census Bureau employees and remaining by the high school students. The digits collected among the Census Bureau employees are easier and cleaner to recognize than the digits collected among the students.\n",
    "\n",
    "\n",
    "### Challenges\n",
    "\n",
    "Now, if you notice the images below, you will find that between 2 characters there are always certain similarities and differences. To teach a machine to recognize these patterns and identify the correct output.\n",
    "\n",
    "![altxt](https://www.researchgate.net/profile/Radu_Tudor_Ionescu/publication/282924675/figure/fig3/AS:319968869666820@1453297931093/A-random-sample-of-6-handwritten-digits-from-the-MNIST-data-set-before-and-after.png)\n",
    "\n",
    "Hence, all these challenges make this a good problem to solve in Machine Learning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZkLmftP9oCeD"
   },
   "source": [
    "## Domain Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M2LBE8FOoGrF"
   },
   "source": [
    "Handwriting changes person to person. Some of us have neat handwriting and some have illegible handwriting such as doctors. However, if you think about it even a child who recognizes alphabets and numerics can identify the characters of a text even written by a stranger. But even a technically knowledgeable adult cannot describe the process by which he or she recognizes the text/letters. As you know this is an excellent challenge for Machine Learning.\n",
    "\n",
    "![altxt](https://i.pinimg.com/originals/f2/7a/ac/f27aac4542c0090872110836d65f4c99.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AbwQ87pLoMAr"
   },
   "source": [
    "## AI /ML Technique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "old2jD8doO0T"
   },
   "source": [
    "### K-Fold Cross Validation\n",
    "\n",
    "\n",
    "The problem with machine learning models is that you won’t get to know how well a model performs until you test it's performance on an independent data set (the data set which was not used for training the machine learning model).\n",
    "\n",
    "Cross Validation comes in to picture here and helps us to estimate the performance of our model. One type of cross validation is the K-Fold Cross Validation\n",
    "\n",
    "In our experiment, we are using the K-Fold Cross Validation technique to reduce (limit) the problem of overfitting. K-Fold Cross Validation is a way to evaluate and improve the performance of our machine learning model. It helps to prevent overfitting to a single train or test split.\n",
    "\n",
    "\n",
    "When we are given a machine learning problem, we will be given two types of data sets — known data (training data set) and unknown data (test data set). By using cross-validation, you would be “testing” your machine learning model in the “training” phase to check for overfitting and to get an idea about how your machine learning model will generalize to independent data, which is the test data set given in the problem.\n",
    "\n",
    "\n",
    "In the first round of cross validation, we have to divide our original training data set into two parts:\n",
    "\n",
    "1. Cross validation training set\n",
    "2. Cross validation testing set or Validation set\n",
    "\n",
    "![alt text](https://cdn.talentsprint.com/aiml/Experiment_related_data/IMAGES/K-Fold.png)\n",
    "\n",
    "The above image represents how the K-Fold Cross Validation works. We divide the dataset in to \"K'' parts and will use the K-1 parts for training and remaining 1 for testing. We will rotate the test set and repeat the process for \"K\" times. \n",
    "\n",
    "we will train our machine learning model on the cross validation training set and test the model’s predictions against the validation set. we will get to know how accurate our machine learning model’s predictions are when we compare the model’s predictions on the validation set and the actual labels of the data points in the validation set.\n",
    "\n",
    "To reduce the variability, multiple rounds of cross validation are performed by using different cross validation training sets and cross validation testing sets. The results from all the rounds are averaged to estimate the accuracy of the machine learning model.\n",
    "\n",
    "**K-fold cross validation is performed as per the following steps:**\n",
    "\n",
    "1. Randomly split the entire training dataset into k subsets.\n",
    "2. Reserve one block as our test data\n",
    "3. Train on each of the remaining K-1 blocks\n",
    "4. Measure the performance against the test set\n",
    "5. The average of our K recorded errors is called the cross-validation error and it will be used as a performance metric for the model\n",
    "\n",
    "For this particular experiment, we have applied a k-foldclassifier() from the sklearn.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "883iWwdPpyth"
   },
   "source": [
    "#### Expected time : 30mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SFzKqwLppPJI"
   },
   "outputs": [],
   "source": [
    "## Importing required packages\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zWoLTkutpPJN"
   },
   "source": [
    "Loading the dataset from sklearn package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d3Zg9n4zpPJO"
   },
   "outputs": [],
   "source": [
    "## Loading MNIST dataset from sklearn\n",
    "digits = datasets.load_digits(n_class=10)\n",
    "## Loding the data and storing in x\n",
    "X = digits.data\n",
    "## Loading the target data and storing it in y\n",
    "y = digits.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IfXTtoTyHeki"
   },
   "source": [
    "### Hyper Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GKvy1plDpPJS"
   },
   "outputs": [],
   "source": [
    "# Activation Functions\n",
    "a = [\"identity\",\"logistic\",\"tanh\",\"relu\"]\n",
    "# Solvers (Optimizers)\n",
    "s = [\"lbfgs\",\"sgd\",\"adam\"]\n",
    "# Learning Rate\n",
    "lr = [0.0001,0.001,0.01,0.1]\n",
    "# Hidden Layers and number of nodes in each layer\n",
    "h = [(5,2),(3,2),(6,3),(7,2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FKanYk0rpPJW"
   },
   "outputs": [],
   "source": [
    "## Applying K-Folds cross-validator\n",
    "kf = KFold(n_splits=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DqWhE0sspPJd"
   },
   "outputs": [],
   "source": [
    "# Function to Create MLP classifier object with hyper parameters\n",
    "def mlp(a,s,h,lr):\n",
    "    clf = MLPClassifier(activation= a ,solver= s ,hidden_layer_sizes = h,learning_rate_init=lr)\n",
    "    return clf  \n",
    "# Function to calculate the accuracy\n",
    "def accuracy(actual,predicted):\n",
    "    return accuracy_score(actual,predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FYi1hdOIOiq-"
   },
   "source": [
    "#### Calculating Training and Testing accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WUKHZALjH8ug"
   },
   "source": [
    "**Note :**  You may ignore the warning below as they are internal to the Sklearn's config and not related to the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "TYeKG-copPJl",
    "outputId": "85d9b32d-3dad-4953-ea71-281496ad3f21"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hyper-parameters = \n",
      " activation =  relu \n",
      " solver =  lbfgs \n",
      " learning_rate_init =  0.1 \n",
      " hidden_layer_sizes =  (6, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of folds is4\n",
      "(train,test) accuracy =  0.3090517117938754 0.2787465973768869\n",
      "\n",
      "Hyper-parameters = \n",
      " activation =  logistic \n",
      " solver =  sgd \n",
      " learning_rate_init =  0.001 \n",
      " hidden_layer_sizes =  (3, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of folds is4\n",
      "(train,test) accuracy =  0.13559035465117558 0.12634001484780996\n",
      "\n",
      "Hyper-parameters = \n",
      " activation =  tanh \n",
      " solver =  lbfgs \n",
      " learning_rate_init =  0.01 \n",
      " hidden_layer_sizes =  (5, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of folds is4\n",
      "(train,test) accuracy =  0.4993768435847107 0.4673694630042069\n",
      "\n",
      "Hyper-parameters = \n",
      " activation =  relu \n",
      " solver =  adam \n",
      " learning_rate_init =  0.0001 \n",
      " hidden_layer_sizes =  (3, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of folds is4\n",
      "(train,test) accuracy =  0.15692719726659307 0.15914377629299678\n",
      "\n",
      "Hyper-parameters = \n",
      " activation =  relu \n",
      " solver =  lbfgs \n",
      " learning_rate_init =  0.1 \n",
      " hidden_layer_sizes =  (5, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of folds is4\n",
      "(train,test) accuracy =  0.35837689645525056 0.327750556792873\n",
      "\n",
      "Hyper-parameters = \n",
      " activation =  identity \n",
      " solver =  sgd \n",
      " learning_rate_init =  0.001 \n",
      " hidden_layer_sizes =  (3, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of folds is4\n",
      "(train,test) accuracy =  0.6555430630547275 0.5698205889631279\n",
      "\n",
      "Hyper-parameters = \n",
      " activation =  tanh \n",
      " solver =  sgd \n",
      " learning_rate_init =  0.001 \n",
      " hidden_layer_sizes =  (5, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of folds is4\n",
      "(train,test) accuracy =  0.3292572074662014 0.29495050730017325\n",
      "\n",
      "Hyper-parameters = \n",
      " activation =  relu \n",
      " solver =  adam \n",
      " learning_rate_init =  0.1 \n",
      " hidden_layer_sizes =  (6, 3)\n",
      "Number of folds is4\n",
      "(train,test) accuracy =  0.10183664545236254 0.09738678544914625\n",
      "\n",
      "Hyper-parameters = \n",
      " activation =  logistic \n",
      " solver =  adam \n",
      " learning_rate_init =  0.001 \n",
      " hidden_layer_sizes =  (7, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of folds is4\n",
      "(train,test) accuracy =  0.36005484767777174 0.324974016332591\n",
      "\n",
      "Hyper-parameters = \n",
      " activation =  relu \n",
      " solver =  lbfgs \n",
      " learning_rate_init =  0.1 \n",
      " hidden_layer_sizes =  (7, 2)\n",
      "Number of folds is4\n",
      "(train,test) accuracy =  0.23243169787129989 0.2097772828507795\n"
     ]
    }
   ],
   "source": [
    "test_accuracy = []\n",
    "train_accuracy = []\n",
    "for i in range(10):\n",
    "    k1 = np.random.randint(0,len(a))\n",
    "    k2 = np.random.randint(0,len(s))\n",
    "    k3 = np.random.randint(0,len(lr))\n",
    "    k4 = np.random.randint(0,len(h))\n",
    "    print(\"\\nHyper-parameters = \\n activation = \", a[k1],    \"\\n solver = \", s[k2], \"\\n learning_rate_init = \", lr[k3],         \"\\n hidden_layer_sizes = \", h[k4])\n",
    "     #calling the mlp function with random hyper paramters\n",
    "    clf = mlp(a[k1],s[k2],h[k4],lr[k3])\n",
    "    tempTrain = 0\n",
    "    tempTest = 0\n",
    "    for nbrOfFolds,(train_index, test_index) in enumerate(kf.split(X)):\n",
    "        ## Splitting the data into train and test\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        Y_train, Y_test  = y[train_index], y[test_index]\n",
    "        ##fit the data into the model\n",
    "        clf.fit(X_train,Y_train)\n",
    "        ##predicting the values on the fitted model using train data\n",
    "        predTrain = clf.predict((X_train))\n",
    "        #adding the accuracy\n",
    "        tempTrain = tempTrain + accuracy(Y_train,predTrain)\n",
    "        ##predict the values on the fitted model using test data\n",
    "        predTest = clf.predict((X_test))\n",
    "        #adding the accuracy\n",
    "        tempTest = tempTest + accuracy(Y_test,predTest)\n",
    "    ##Calculating the train accuracy\n",
    "    print(f'Number of folds is{nbrOfFolds+1}')\n",
    "    train_accuracy.append(tempTrain*1.0/(nbrOfFolds+1))\n",
    "    ##Calculating the test accuracy\n",
    "    test_accuracy.append(tempTest*1.0/(nbrOfFolds+1))\n",
    "    print(\"(train,test) accuracy = \",tempTrain*1.0/(nbrOfFolds+1), tempTest*1.0/(nbrOfFolds+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "colab_type": "code",
    "id": "uMAhMltBjEch",
    "outputId": "dceba75a-25a7-4009-a947-f94b6c232aeb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.2787465973768869,\n",
       "  0.12634001484780996,\n",
       "  0.4673694630042069,\n",
       "  0.15914377629299678,\n",
       "  0.327750556792873,\n",
       "  0.5698205889631279,\n",
       "  0.29495050730017325,\n",
       "  0.09738678544914625,\n",
       "  0.324974016332591,\n",
       "  0.2097772828507795],\n",
       " [0.3090517117938754,\n",
       "  0.13559035465117558,\n",
       "  0.4993768435847107,\n",
       "  0.15692719726659307,\n",
       "  0.35837689645525056,\n",
       "  0.6555430630547275,\n",
       "  0.3292572074662014,\n",
       "  0.10183664545236254,\n",
       "  0.36005484767777174,\n",
       "  0.23243169787129989])"
      ]
     },
     "execution_count": 11,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_accuracy, train_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6baqLI7VpPJr"
   },
   "source": [
    "#### Plotting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "colab_type": "code",
    "id": "Z_CMEv2apPJt",
    "outputId": "0ead7a92-559b-4bcc-ffe6-56c39af4e017"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAASLUlEQVR4nO3df5BV5X3H8ffXBYpRIi1uTcqCy1iq\nwVgNbjVqJslENCgd6Exi1cRMQrQ77YRqTdJ2nXYMgzMdTDtpncgkYQzWaROJ1aTdVgw1PzqZaRoF\nDf4AJG4JlbVYgSjaJgYJ3/6xV3Kz7rKX5e49y7Pv1wzDfc557j3fe2f2s899zjnPRmYiSTr2HVd1\nAZKk5jDQJakQBrokFcJAl6RCGOiSVIhJVR345JNPzs7OzqoOL0nHpEceeWRPZrYPta+yQO/s7GTj\nxo1VHV6SjkkR8V/D7XPKRZIKYaBLUiEMdEkqRGVz6JLUqFdffZX+/n5eeeWVqktpmalTp9LR0cHk\nyZMbfo6BLmnc6+/vZ9q0aXR2dhIRVZcz5jKTvXv30t/fz5w5cxp+nlMuksa9V155hRkzZkyIMAeI\nCGbMmHHE30gMdEnHhIkS5q8Zzfs10CWpEM6hSzrmdPbc39TX27Fy0WH37927l4svvhiA5557jra2\nNtrbB27WfPjhh5kyZcqIx1i6dCk9PT2cfvrpR1/wMAx0aZQOFyojBYSOLTNmzGDTpk0ALF++nBNP\nPJFPfvKTv9AnM8lMjjtu6ImPO++8c8zrdMpFkkapr6+PefPm8cEPfpAzzzyTXbt20d3dTVdXF2ee\neSYrVqw41Pcd73gHmzZt4sCBA0yfPp2enh7OPvtsLrjgAp5//vmm1GOgS9JReOqpp7jxxhvZsmUL\nM2fOZOXKlWzcuJHHHnuMBx98kC1btrzuOfv27eNd73oXjz32GBdccAFr1qxpSi0GuiQdhdNOO42u\nrq5D7bvvvpv58+czf/58tm7dOmSgH3/88Vx22WUAnHvuuezYsaMptTiHLklH4YQTTjj0+Omnn+a2\n227j4YcfZvr06VxzzTVDXktefxK1ra2NAwcONKUWR+iS1CQvvfQS06ZN441vfCO7du1i/fr1LT2+\nI3RJx5zxehXR/PnzmTdvHmeccQannnoqF110UUuPH5nZ0gO+pqurK/0DFzqWedli62zdupW3vOUt\nVZfRckO974h4JDO7hurvlIskFcJAl6RCGOiSVAgDXZIKYaBLUiEMdEkqREPXoUfEQuA2oA24IzNX\nDtHnd4HlQAKPZeYHmlinJP3c8pOa/Hr7Dru7GcvnAqxZs4bLL7+cN73pTUdX7zBGDPSIaANWAZcA\n/cCGiOjNzC11feYCNwEXZeYLEfGrY1KtJFWgkeVzG7FmzRrmz59fXaAD5wF9mbkdICLWAkuA+hVn\nfg9YlZkvAGRmc9aClKRx7q677mLVqlXs37+fCy+8kNtvv52DBw+ydOlSNm3aRGbS3d3NKaecwqZN\nm7jyyis5/vjjj2hk36hGAn0msLOu3Q+cP6jPbwBExL8zMC2zPDO/PviFIqIb6AaYPXv2aOqVpHHj\nySef5Gtf+xrf/e53mTRpEt3d3axdu5bTTjuNPXv28MQTTwDw4osvMn36dD772c9y++23c84554xJ\nPc1ay2USMBd4N9ABfCcizsrMF+s7ZeZqYDUM3PrfpGNLUiW+8Y1vsGHDhkPL5/7kJz9h1qxZvPe9\n72Xbtm1cf/31LFq0iEsvvbQl9TQS6M8Cs+raHbVt9fqBhzLzVeCHEfEDBgJ+Q1OqlKRxKDP56Ec/\nyi233PK6fY8//jgPPPAAq1at4r777mP16tVjXk8jly1uAOZGxJyImAJcBfQO6vOPDIzOiYiTGZiC\n2d7EOiVp3FmwYAH33HMPe/bsAQauhnnmmWfYvXs3mckVV1zBihUrePTRRwGYNm0aL7/88pjVM+II\nPTMPRMQyYD0D8+NrMnNzRKwANmZmb23fpRGxBfgZ8MeZuXfMqpY0sY1wmWGrnHXWWXzqU59iwYIF\nHDx4kMmTJ/P5z3+etrY2rr32WjKTiODWW28FYOnSpVx33XVjdlLU5XOlUXL53NZx+dyfc/lcSZoA\nDHRJKoSBLumYUNX0cFVG834NdEnj3tSpU9m7d++ECfXMZO/evUydOvWInucfiZbGwnCLR42TqzOO\nNR0dHfT397N79+6qS2mZqVOn0tHRcUTPMdAljXuTJ09mzpw5VZcx7jnlIkmFMNAlqRAGuiQVwkCX\npEIY6JJUCANdkgphoEtSIQx0SSqEgS5JhTDQJakQBrokFcJAl6RCGOiSVAgDXZIKYaBLUiEMdEkq\nhIEuSYVoKNAjYmFEbIuIvojoGWL/RyJid0Rsqv27rvmlSpIOZ8Q/QRcRbcAq4BKgH9gQEb2ZuWVQ\n169k5rIxqFGS1IBGRujnAX2ZuT0z9wNrgSVjW5Yk6Ug18keiZwI769r9wPlD9HtfRLwT+AFwY2bu\nHNwhIrqBboDZs2cfebU6pLPn/mH37Vi5qIWVSBovmnVS9J+Bzsz8TeBB4K6hOmXm6szsysyu9vb2\nJh1akgSNBfqzwKy6dkdt2yGZuTczf1pr3gGc25zyJEmNaiTQNwBzI2JOREwBrgJ66ztExJvrmouB\nrc0rUZLUiBHn0DPzQEQsA9YDbcCazNwcESuAjZnZC1wfEYuBA8CPgI+MYc2SpCE0clKUzFwHrBu0\n7ea6xzcBNzW3NEnSkfBOUUkqhIEuSYUw0CWpEAa6JBWioZOiOsYsP2mY7ftaW4eklnKELkmFMNAl\nqRAGuiQVwkCXpEIY6JJUCANdkgphoEtSIQx0SSqEgS5JhTDQJakQBrokFcJAl6RCGOiSVAgDXZIK\nYaBLUiEMdEkqhIEuSYVoKNAjYmFEbIuIvojoOUy/90VERkRX80qUJDVixECPiDZgFXAZMA+4OiLm\nDdFvGnAD8FCzi5QkjayREfp5QF9mbs/M/cBaYMkQ/W4BbgVeaWJ9kqQGNRLoM4Gdde3+2rZDImI+\nMCsz7z/cC0VEd0RsjIiNu3fvPuJiJUnDO+qTohFxHPAZ4BMj9c3M1ZnZlZld7e3tR3toSVKdRgL9\nWWBWXbujtu0104C3Av8WETuAtwO9nhiVpNZqJNA3AHMjYk5ETAGuAnpf25mZ+zLz5MzszMxO4HvA\n4szcOCYVS5KGNGKgZ+YBYBmwHtgK3JOZmyNiRUQsHusCJUmNmdRIp8xcB6wbtO3mYfq+++jLkiQd\nKe8UlaRCGOiSVAgDXZIKYaBLUiEMdEkqhIEuSYUw0CWpEA1dhy5J41Fnz/DrAe5YuaiFlYwPBrqO\nWcP9ME/EH2QJnHKRpGIY6JJUCANdkgphoEtSIQx0SSqEgS5JhTDQJakQBrokFcIbi1Se5ScdZt++\n1tXRAt5cpXoGuqQyDfeLvbBf6vWccpGkQhjoklQIA12SCmGgS1IhGjopGhELgduANuCOzFw5aP/v\nAx8Dfgb8L9CdmVuaXOshroEsSa834gg9ItqAVcBlwDzg6oiYN6jblzPzrMw8B/g08JmmVypJOqxG\nplzOA/oyc3tm7gfWAkvqO2TmS3XNE4BsXomSpEY0MuUyE9hZ1+4Hzh/cKSI+BnwcmAK8Z6gXiohu\noBtg9uzZR1qrJOkwmnZSNDNXZeZpwJ8Cfz5Mn9WZ2ZWZXe3t7c06tCSJxkbozwKz6todtW3DWQt8\n7miKknSUJtDyB/q5RkboG4C5ETEnIqYAVwG99R0iYm5dcxHwdPNKlCQ1YsQRemYeiIhlwHoGLltc\nk5mbI2IFsDEze4FlEbEAeBV4AfjwWBYtSXq9hq5Dz8x1wLpB226ue3xDk+uSJB0h7xSVpEKUt3zu\nBFwyU5LAEbokFcNAl6RCGOiSVAgDXZIKYaBLUiEMdEkqhIEuSYUw0CWpEAa6JBXCQJekQhjoklQI\nA12SCmGgS1IhylttUZJaqLPn/iG371i5qMWVOEKXpGIY6JJUCANdkgphoEtSIQx0SSqEgS5JhWgo\n0CNiYURsi4i+iOgZYv/HI2JLRDweEd+MiFObX6ok6XBGvA49ItqAVcAlQD+wISJ6M3NLXbfvA12Z\n+eOI+APg08CVY1GwJB0Tlp90mH37xuSQjYzQzwP6MnN7Zu4H1gJL6jtk5rcz88e15veAjuaWKUka\nSSOBPhPYWdfur20bzrXAA0dTlCTpyDX11v+IuAboAt41zP5uoBtg9uzZzTy0JE14jYzQnwVm1bU7\natt+QUQsAP4MWJyZPx3qhTJzdWZ2ZWZXe3v7aOqVJA2jkUDfAMyNiDkRMQW4Cuit7xARbwO+wECY\nP9/8MiVJIxkx0DPzALAMWA9sBe7JzM0RsSIiFte6/SVwIvAPEbEpInqHeTlJ0hhpaA49M9cB6wZt\nu7nu8YIm1yVJOkLeKSpJhTDQJakQBrokFcJAl6RCGOiSVAgDXZIKYaBLUiEMdEkqRFMX59LE0dlz\n/7D7dkz9wNA7xmgNaEkDHKFLUiEMdEkqhIEuSYUw0CWpEAa6JBXCQJekQnjZ4igNd9nejpWLWlyJ\nJA1whC5JhTDQJakQTrk02/KTDrPPOyUljR1H6JJUCANdkgphoEtSIQx0SSpEQ4EeEQsjYltE9EVE\nzxD73xkRj0bEgYh4f/PLlCSNZMRAj4g2YBVwGTAPuDoi5g3q9gzwEeDLzS5QktSYRi5bPA/oy8zt\nABGxFlgCbHmtQ2buqO07OAY1SpIa0EigzwR21rX7gfNHc7CI6Aa6AWbPnj2al5A0jrgExvjS0huL\nMnM1sBqgq6srW3lsSS3kDXaVaOSk6LPArLp2R22bJGkcaSTQNwBzI2JOREwBrgJ6x7YsSdKRGjHQ\nM/MAsAxYD2wF7snMzRGxIiIWA0TEb0VEP3AF8IWI2DyWRUuSXq+hOfTMXAesG7Tt5rrHGxiYipEk\nVcQ7RSWpEAa6JBXCQJekQhjoklQIA12SCmGgS1IhDHRJKoSBLkmFMNAlqRAGuiQVwkCXpEIY6JJU\nCANdkgphoEtSIQx0SSqEgS5JhTDQJakQBrokFcJAl6RCGOiSVAgDXZIKYaBLUiEMdEkqREOBHhEL\nI2JbRPRFRM8Q+38pIr5S2/9QRHQ2u1BJ0uGNGOgR0QasAi4D5gFXR8S8Qd2uBV7IzF8H/hq4tdmF\nSpIOr5ER+nlAX2Zuz8z9wFpgyaA+S4C7ao/vBS6OiGhemZKkkURmHr5DxPuBhZl5Xa39IeD8zFxW\n1+fJWp/+Wvs/a332DHqtbqC71jwd2NasN1Kxk4E9I/Yq20T/DCb6+wc/A2jNZ3BqZrYPtWPSGB/4\nF2TmamB1K4/ZChGxMTO7qq6jShP9M5jo7x/8DKD6z6CRKZdngVl17Y7atiH7RMQk4CRgbzMKlCQ1\nppFA3wDMjYg5ETEFuAroHdSnF/hw7fH7gW/lSHM5kqSmGnHKJTMPRMQyYD3QBqzJzM0RsQLYmJm9\nwBeBv4uIPuBHDIT+RFLcNNIoTPTPYKK/f/AzgIo/gxFPikqSjg3eKSpJhTDQJakQBvooRcSsiPh2\nRGyJiM0RcUPVNVUlItoi4vsR8S9V11KFiJgeEfdGxFMRsTUiLqi6plaLiBtrPwdPRsTdETG16prG\nWkSsiYjna/fhvLbtVyLiwYh4uvb/L7eyJgN99A4An8jMecDbgY8NsSTCRHEDsLXqIip0G/D1zDwD\nOJsJ9llExEzgeqArM9/KwMUTE+HCiL8FFg7a1gN8MzPnAt+stVvGQB+lzNyVmY/WHr/MwA/xzGqr\nar2I6AAWAXdUXUsVIuIk4J0MXOlFZu7PzBerraoSk4Dja/ehvAH474rrGXOZ+R0GruqrV78Myl3A\n77SyJgO9CWqrS74NeKjaSirxN8CfAAerLqQic4DdwJ21aac7IuKEqotqpcx8Fvgr4BlgF7AvM/+1\n2qoqc0pm7qo9fg44pZUHN9CPUkScCNwH/FFmvlR1Pa0UEb8NPJ+Zj1RdS4UmAfOBz2Xm24D/o8Vf\ns6tWmydewsAvt18DToiIa6qtqnq1mytbel24gX4UImIyA2H+pcz8atX1VOAiYHFE7GBgFc73RMTf\nV1tSy/UD/Zn52rezexkI+IlkAfDDzNydma8CXwUurLimqvxPRLwZoPb/8608uIE+SrXlgb8IbM3M\nz1RdTxUy86bM7MjMTgZOgn0rMyfUyCwznwN2RsTptU0XA1sqLKkKzwBvj4g31H4uLmaCnRiuU78M\nyoeBf2rlwQ300bsI+BADo9JNtX+XV12UKvGHwJci4nHgHOAvKq6npWrfTu4FHgWeYCBXil8GICLu\nBv4DOD0i+iPiWmAlcElEPM3AN5eVLa3JW/8lqQyO0CWpEAa6JBXCQJekQhjoklQIA12SCmGgS1Ih\nDHRJKsT/A/ne3/GxnR2ZAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "##Plotting the data\n",
    "xx = np.array(range(1,11))\n",
    "plt.bar(xx,train_accuracy,width=0.2)\n",
    "plt.bar(xx+0.2, test_accuracy,width=0.2)\n",
    "plt.legend([\"Train\",\"Test\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "skiIgURmpPJx"
   },
   "source": [
    "#### Ungraded Exercise \n",
    "\n",
    "Vary the number of k-fold splits and observe the changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "677BLEdKQ_Mg",
    "outputId": "478e4ff7-371b-454c-db26-5a23c3bed7e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hyper-parameters = \n",
      " activation =  tanh \n",
      " solver =  sgd \n",
      " learning_rate_init =  0.1 \n",
      " hidden_layer_sizes =  (5, 2)\n",
      "Number of folds is4\n",
      "(train,test) accuracy =  0.5080790040071463 0.48743999010146\n",
      "\n",
      "Hyper-parameters = \n",
      " activation =  tanh \n",
      " solver =  sgd \n",
      " learning_rate_init =  0.0001 \n",
      " hidden_layer_sizes =  (6, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of folds is4\n",
      "(train,test) accuracy =  0.16971567214978223 0.16363771343726802\n",
      "\n",
      "Hyper-parameters = \n",
      " activation =  relu \n",
      " solver =  sgd \n",
      " learning_rate_init =  0.001 \n",
      " hidden_layer_sizes =  (6, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of folds is4\n",
      "(train,test) accuracy =  0.3902615494592886 0.34340262311309083\n",
      "\n",
      "Hyper-parameters = \n",
      " activation =  identity \n",
      " solver =  adam \n",
      " learning_rate_init =  0.0001 \n",
      " hidden_layer_sizes =  (6, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of folds is4\n",
      "(train,test) accuracy =  0.3392659861787597 0.3244394951744618\n",
      "\n",
      "Hyper-parameters = \n",
      " activation =  logistic \n",
      " solver =  lbfgs \n",
      " learning_rate_init =  0.001 \n",
      " hidden_layer_sizes =  (7, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of folds is4\n",
      "(train,test) accuracy =  0.4857907670413866 0.43024003959415985\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=4)\n",
    "\n",
    "test_accuracy = []\n",
    "train_accuracy = []\n",
    "for i in range(5):\n",
    "    k1 = np.random.randint(0,len(a))\n",
    "    k2 = np.random.randint(0,len(s))\n",
    "    k3 = np.random.randint(0,len(lr))\n",
    "    k4 = np.random.randint(0,len(h))\n",
    "    print(\"\\nHyper-parameters = \\n activation = \", a[k1],    \"\\n solver = \", s[k2], \"\\n learning_rate_init = \", lr[k3],         \"\\n hidden_layer_sizes = \", h[k4])\n",
    "     #calling the mlp function with random hyper paramters\n",
    "    clf = mlp(a[k1],s[k2],h[k4],lr[k3])\n",
    "    tempTrain = 0\n",
    "    tempTest = 0\n",
    "    for nbrOfFolds,(train_index, test_index) in enumerate(kf.split(X)):\n",
    "        ## Splitting the data into train and test\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        Y_train, Y_test  = y[train_index], y[test_index]\n",
    "        ##fit the data into the model\n",
    "        clf.fit(X_train,Y_train)\n",
    "        ##predicting the values on the fitted model using train data\n",
    "        predTrain = clf.predict((X_train))\n",
    "        #adding the accuracy\n",
    "        tempTrain = tempTrain + accuracy(Y_train,predTrain)\n",
    "        ##predict the values on the fitted model using test data\n",
    "        predTest = clf.predict((X_test))\n",
    "        #adding the accuracy\n",
    "        tempTest = tempTest + accuracy(Y_test,predTest)\n",
    "    ##Calculating the train accuracy\n",
    "    print(f'Number of folds is{nbrOfFolds+1}')\n",
    "    train_accuracy.append(tempTrain*1.0/(nbrOfFolds+1))\n",
    "    ##Calculating the test accuracy\n",
    "    test_accuracy.append(tempTest*1.0/(nbrOfFolds+1))\n",
    "    print(\"(train,test) accuracy = \",tempTrain*1.0/(nbrOfFolds+1), tempTest*1.0/(nbrOfFolds+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "GRNFvL3QqDcJ",
    "outputId": "c94f3b73-97b8-432c-94ef-3a85a2b69b1a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.48743999010146,\n",
       "  0.16363771343726802,\n",
       "  0.34340262311309083,\n",
       "  0.3244394951744618,\n",
       "  0.43024003959415985],\n",
       " [0.5080790040071463,\n",
       "  0.16971567214978223,\n",
       "  0.3902615494592886,\n",
       "  0.3392659861787597,\n",
       "  0.4857907670413866])"
      ]
     },
     "execution_count": 14,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_accuracy, train_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "colab_type": "code",
    "id": "pbwH5IxqqD9H",
    "outputId": "acbbd5b7-53b5-4bd9-aa42-8f5756a2c5f2"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n9kkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "##Plotting the data\n",
    "i = np.array(range(1,6))\n",
    "plt.bar(i,train_accuracy,width=0.3)\n",
    "plt.bar(i+0.2, test_accuracy,width=0.3)\n",
    "plt.legend([\"Train\",\"Test\"])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "017_Kfold_Validation_C.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
